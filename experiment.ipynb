{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d248d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2021 Google Health Research.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Lint as: python3\n",
    "\"\"\"Experiment runner.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c8a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from absl import app\n",
    "from absl import logging\n",
    "from ehr_prediction_modeling import config as experiment_config\n",
    "from ehr_prediction_modeling import embeddings\n",
    "from ehr_prediction_modeling import encoder_module_base\n",
    "from ehr_prediction_modeling import losses\n",
    "from ehr_prediction_modeling import types\n",
    "from ehr_prediction_modeling.data import tf_dataset\n",
    "from ehr_prediction_modeling.eval import metrics_coordinator as metrics\n",
    "from ehr_prediction_modeling.models import model_utils\n",
    "from ehr_prediction_modeling.models import rnn_model\n",
    "from ehr_prediction_modeling.models import snrnn_model\n",
    "from ehr_prediction_modeling.tasks import coordinator\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb12278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary containing the full set of hyperparameters\n",
    "config = experiment_config.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98114052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_from_config(config, task_name):\n",
    "    \"\"\"Returns an instantiated Task based on the provided task name.\"\"\"\n",
    "    if task_name not in config.task_configs:\n",
    "        raise ValueError(\n",
    "            \"Task %s is not present in the list of task configurations: %s.\" %\n",
    "            (task_name, config.task_configs.keys()))\n",
    "    task_config = config.task_configs[task_name]\n",
    "    if task_config.task_type not in experiment_config.TASK_MAPPING:\n",
    "        raise ValueError(\"config.tasks.type unknown: %s\" % task_config.task_type)\n",
    "        \n",
    "    #TASK_MAPPING is a dictionary of task classes: labs, LoS, mortality, etc.\n",
    "    task = experiment_config.TASK_MAPPING[task_config.task_type](task_config)\n",
    "    return task\n",
    "\n",
    "def get_task_coordinator(config):\n",
    "    task_list = [\n",
    "        get_task_from_config(config, task_name) for task_name in config.tasks\n",
    "    ]\n",
    "    return coordinator.Coordinator(task_list, optimizer_config=config.get(\"optimizer\"))\n",
    "\n",
    "task_coordinator = get_task_coordinator(config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510705c5-0d6f-42d6-ad75-da83a84cae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_random_seed(config.get(\"seed\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef15bee7-6037-445b-866e-47bbbcf2edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4d8d8-32ff-42c3-bc2a-a8771a8d90d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_coordinator = metrics.MetricsCoordinator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa43b7-ccd6-4312-a1ea-d354339dd4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_classes = {\n",
    "    types.EmbeddingType.LOOKUP: embeddings.BasicEmbeddingLookup,\n",
    "    types.EmbeddingType.DEEP_EMBEDDING: embeddings.DeepEmbedding\n",
    "}\n",
    "\n",
    "encoder = encoder_module_base.EncoderModule(config.encoder, embedding_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f733909a-37ad-4c65-b5e2-b32a8dbc8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_init_kwargs = {\n",
    "#     \"config\": config.model,\n",
    "#     \"embedding_size\": encoder.get_total_embedding_size()\n",
    "# }\n",
    "# base_model = rnn_model.RNNModel(**model_init_kwargs)\n",
    "\n",
    "\n",
    "\n",
    "model_init_kwargs = {\n",
    "    \"config\": config.model,\n",
    "    \"tasks\": config.tasks,\n",
    "    \"embedding_size\": encoder.get_total_embedding_size()\n",
    "}\n",
    "\n",
    "base_model = snrnn_model.SNRNNModel(**model_init_kwargs)\n",
    "model = model_utils.RNNModelWithPersistentState(base_model)\n",
    "optimizer = model_utils.get_optimizer_from_config(config.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b979d6-477e-45e9-92e5-914f5b8d6de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize how batches will be generated\n",
    "batch_gen = tf_dataset.BatchGenerator(config, True, task_coordinator, \"train\")\n",
    "batch = batch_gen.batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050241b3-8109-47ba-b5ca-1f90a99d15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode batches into embedding\n",
    "features, time_vect = encoder.embed_batch(batch)\n",
    "forward_return = model(features, batch.is_beginning_sequence, time_vect)\n",
    "tasks_graph = task_coordinator.get_coordinator_variables(batch, forward_return.model_output)\n",
    "embedding_loss, _ = encoder.get_embedding_loss(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59783d4f-136b-49c9-8383-b1ed04d227c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tasks_graph.combined_loss\n",
    "loss += encoder.get_embedding_regularization_loss()\n",
    "loss += embedding_loss\n",
    "loss += model.get_model_regularization_loss()\n",
    "\n",
    "losses_per_task = {}\n",
    "for task_name, task_vars in zip(task_coordinator.task_names, tasks_graph.task_variables_list):\n",
    "    losses_per_task[task_name] = task_vars.loss\n",
    "\n",
    "loss += task_coordinator.get_task_regularization_losses()\n",
    "\n",
    "loss_to_vars = losses.get_loss_to_variables_dict(\n",
    "    model=model,\n",
    "    encoder=encoder,\n",
    "    losses_per_task=losses_per_task,\n",
    "    all_variables=tf.trainable_variables(),\n",
    "    total_loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9983737-32cc-4c0a-a698-c505f2486ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = model_utils.multiple_loss_optim_fn(optimizer, loss_to_vars, norm_clip=config.optimizer.norm_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc8c1b-3844-4576-bd0c-fd583bbdc7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = config.splits_to_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9d5cb8-5f62-4dda-9ae9-24927514e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_eval(config, task_coordinator, split, model, encoder):\n",
    "    batch_gen = tf_dataset.BatchGenerator(config, False, task_coordinator, split)\n",
    "    batch = batch_gen.batch\n",
    "    features, time_vect = encoder.embed_batch(batch)\n",
    "    forward_return = model(features, batch.is_beginning_sequence, time_vect)\n",
    "    tasks_graph = task_coordinator.get_coordinator_variables(batch, forward_return.model_output)\n",
    "    return (batch_gen, tasks_graph.task_variables_list)\n",
    "\n",
    "eval_batch_gen, eval_task_vars = setup_eval(config, task_coordinator, split, model, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730c606-0a4a-45b1-9990-ef0c899a42a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_dir(config, mode):\n",
    "    ttl = \"ttl=%sd\" % config.ttl\n",
    "    return os.path.join(config.checkpoint_dir, \"checkpoints\", ttl, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9129e33c-107a-41b5-b1bc-ca1364b45bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.control_dependencies([step]):\n",
    "    scalar_loss = tf.reduce_mean(loss)\n",
    "    step_cnt = tf.train.get_or_create_global_step()\n",
    "    current_step = 0\n",
    "    \n",
    "checkpoint_dir = get_checkpoint_dir(config.checkpoint, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88035db-e053-42c8-823d-f7dec742c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn_on_data_split(task_coordinator, metrics_coordinator, session,\n",
    "                          eval_batch_gen, eval_task_vars, split_name,\n",
    "                          current_step):\n",
    "  \"\"\"Runs evaluation of a given datasplit and logs metrics.\"\"\"\n",
    "  # The dataset needs to be re-initialized for each epoch since we iterate the\n",
    "  # entire data split.\n",
    "  session.run(eval_batch_gen.iterator.initializer)\n",
    "  task_prediction_types = task_coordinator.task_prediction_types\n",
    "  target_names_list = task_coordinator.target_names_list\n",
    "  fetches = {\n",
    "      \"task_variables_list\": eval_task_vars,\n",
    "  }\n",
    "\n",
    "  batch_count = 0\n",
    "  while True:\n",
    "    try:\n",
    "      fetches_np = session.run(fetches)\n",
    "      for (target_names, task_type,\n",
    "           task_variables) in zip(target_names_list, task_prediction_types,\n",
    "                                  fetches_np[\"task_variables_list\"]):\n",
    "        if task_type == types.TaskType.BINARY_CLASSIFICATION:\n",
    "          metrics.add_batch_to_binary_metrics_data(\n",
    "              metrics_coordinator=metrics_coordinator,\n",
    "              target_names=target_names,\n",
    "              predictions=task_variables.predictions,\n",
    "              binary_targets=task_variables.targets,\n",
    "              eval_mask_dict=task_variables.eval_mask_dict,\n",
    "              split_name=split_name)\n",
    "\n",
    "        elif task_type == types.TaskType.REGRESSION:\n",
    "          metrics.add_batch_to_regression_metrics_data(\n",
    "              metrics_coordinator=metrics_coordinator,\n",
    "              target_names=target_names,\n",
    "              predictions=task_variables.predictions,\n",
    "              targets=task_variables.targets,\n",
    "              eval_mask_dict=task_variables.eval_mask_dict,\n",
    "              split_name=split_name)\n",
    "\n",
    "        else:\n",
    "          raise ValueError(\"Unsupported task type for evaluation: %s\" %\n",
    "                           task_type)\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      # OutOfRangeError is the normal error thrown when the queue is empty\n",
    "      # due to the epoch limitation.\n",
    "      break\n",
    "    batch_count += 1\n",
    "\n",
    "  logging.info(\"Evaluated %s batches.\", batch_count)\n",
    "  metrics_coordinator.log_metrics(current_step, clear_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e96548d-fe0e-4e34-9b9f-718792df4dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.train.MonitoredTrainingSession(\n",
    "    is_chief=True,\n",
    "    # checkpoint_dir=checkpoint_dir,\n",
    "    # save_checkpoint_secs=config.checkpoint.checkpoint_every,\n",
    "    # save_summaries_steps=None,\n",
    "    # save_summaries_secs=None,\n",
    "    config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    ") as session:\n",
    "    fetches = {\n",
    "        \"step\": step_cnt,\n",
    "        \"loss\": scalar_loss,\n",
    "    }\n",
    "    while current_step < config.model.num_steps:\n",
    "        fetches_np = session.run(fetches)\n",
    "        current_step = fetches_np[\"step\"]\n",
    "        print(current_step)\n",
    "        if current_step % 100 == 0:\n",
    "            logging.info(\"step %s, fetches: %s\", current_step, fetches_np)\n",
    "            logging.info(\"Starting evaluation on data split: %s\", split)\n",
    "            eval_fn_on_data_split(task_coordinator, metrics_coordinator, session, eval_batch_gen, eval_task_vars, split, current_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7951fac-f51e-42ba-be88-bfd12eead32d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
